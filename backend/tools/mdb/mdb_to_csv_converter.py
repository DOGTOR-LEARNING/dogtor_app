#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MDB to CSV è½‰æ›å™¨
æ”¯æ´æ‰¹æ¬¡è™•ç†å¤šå€‹ MDB æª”æ¡ˆä¸¦è½‰æ›ç‚º CSV æ ¼å¼
é©ç”¨æ–¼ macOS ç³»çµ±ï¼Œéœ€è¦å®‰è£ mdbtools

å®‰è£èªªæ˜ï¼š
1. å®‰è£ Homebrew (å¦‚æœé‚„æ²’å®‰è£)ï¼š
   /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

2. å®‰è£ mdbtools å’Œ libmagicï¼š
   brew install mdbtools libmagic

3. å®‰è£æ‰€éœ€çš„ Python å¥—ä»¶ï¼š
   pip3 install pandas tqdm chardet python-magic

ä½¿ç”¨æ–¹æ³•ï¼š
python3 mdb_to_csv_converter.py --input_dir /path/to/mdb/files --output_dir /path/to/csv/output
"""

import os
import sys
import subprocess
import argparse
import csv
import logging
from pathlib import Path
from tqdm import tqdm
import pandas as pd
import chardet
from concurrent.futures import ThreadPoolExecutor, as_completed
import tempfile
import zipfile
import io
import base64
import magic
import json
from datetime import datetime

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.DEBUG,  # æ”¹ç‚º DEBUG ç´šåˆ¥ä»¥é¡¯ç¤ºæ›´å¤šè³‡è¨Š
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('mdb_conversion.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class MDBToCSVConverter:
    def __init__(self, input_dir, output_dir, max_workers=2, chunk_size=10000):
        """
        åˆå§‹åŒ–è½‰æ›å™¨
        
        Args:
            input_dir (str): MDB æª”æ¡ˆè¼¸å…¥ç›®éŒ„
            output_dir (str): CSV æª”æ¡ˆè¼¸å‡ºç›®éŒ„
            max_workers (int): æœ€å¤§ä½µç™¼æ•¸
            chunk_size (int): è™•ç†å¤§è³‡æ–™æ™‚çš„åˆ†å¡Šå¤§å°
        """
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.extracted_files_dir = self.output_dir / "extracted_files"  # å­˜æ”¾è§£å£“ç¸®æª”æ¡ˆçš„ç›®éŒ„
        self.max_workers = max_workers
        self.chunk_size = chunk_size
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.extracted_files_dir.mkdir(parents=True, exist_ok=True)
        
        # æª¢æŸ¥ mdbtools å’Œ magic æ˜¯å¦å·²å®‰è£
        self._check_dependencies()
    
    def _check_dependencies(self):
        """æª¢æŸ¥å¿…è¦çš„ä¾è³´å¥—ä»¶æ˜¯å¦å·²å®‰è£"""
        # æª¢æŸ¥ mdbtools
        try:
            subprocess.run(['mdb-tables', '--version'], capture_output=True, check=True)
            logger.info("âœ… mdbtools å·²å®‰è£ä¸¦å¯ç”¨")
        except (subprocess.CalledProcessError, FileNotFoundError):
            logger.error("âŒ mdbtools æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ: brew install mdbtools")
            sys.exit(1)
        
        # æª¢æŸ¥ python-magic
        try:
            import magic
            # æ¸¬è©¦ magic æ˜¯å¦æ­£å¸¸å·¥ä½œ
            magic.from_buffer(b"test", mime=True)
            logger.info("âœ… python-magic å·²å®‰è£ä¸¦å¯ç”¨")
        except ImportError:
            logger.error("âŒ python-magic æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ: pip3 install python-magic")
            sys.exit(1)
        except Exception as e:
            logger.error(f"âŒ python-magic å®‰è£ä¸å®Œæ•´ï¼Œè«‹åŸ·è¡Œ: brew install libmagic ç„¶å¾Œ pip3 install python-magic")
            logger.error(f"è©³ç´°éŒ¯èª¤: {e}")
            sys.exit(1)
    
    def _get_mdb_files(self):
        """å–å¾—æ‰€æœ‰ MDB æª”æ¡ˆ"""
        mdb_files = list(self.input_dir.glob("*.mdb")) + list(self.input_dir.glob("*.MDB"))
        if not mdb_files:
            logger.warning(f"åœ¨ {self.input_dir} ä¸­æ‰¾ä¸åˆ° MDB æª”æ¡ˆ")
        return mdb_files
    
    def _get_table_list(self, mdb_file):
        """å–å¾— MDB æª”æ¡ˆä¸­çš„æ‰€æœ‰è¡¨æ ¼åç¨±"""
        try:
            result = subprocess.run(
                ['mdb-tables', str(mdb_file)],
                capture_output=True,
                text=True,
                check=True
            )
            tables = [table.strip() for table in result.stdout.split() if table.strip()]
            return tables
        except subprocess.CalledProcessError as e:
            logger.error(f"ç„¡æ³•è®€å– {mdb_file} çš„è¡¨æ ¼åˆ—è¡¨: {e}")
            return []
    
    def _export_table_to_csv(self, mdb_file, table_name, output_file):
        """å°‡å–®å€‹è¡¨æ ¼åŒ¯å‡ºç‚º CSV"""
        try:
            # ä½¿ç”¨ mdb-export åŒ¯å‡ºè³‡æ–™
            process = subprocess.Popen(
                ['mdb-export', str(mdb_file), table_name],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=False  # ä½¿ç”¨ bytes æ¨¡å¼é¿å…ç·¨ç¢¼å•é¡Œ
            )
            
            # å–å¾—åŸå§‹è¼¸å‡º
            stdout_data, stderr_data = process.communicate()
            
            if process.returncode != 0:
                error_msg = stderr_data.decode('utf-8', errors='replace')
                logger.error(f"âŒ åŒ¯å‡º {table_name} å¤±æ•—: {error_msg}")
                return False
            
            # åµæ¸¬ç·¨ç¢¼
            detected_encoding = chardet.detect(stdout_data)
            encoding = detected_encoding.get('encoding', 'utf-8')
            confidence = detected_encoding.get('confidence', 0)
            
            logger.info(f"åµæ¸¬åˆ°ç·¨ç¢¼: {encoding} (ä¿¡å¿ƒåº¦: {confidence:.2f})")
            
            # å˜—è©¦ä¸åŒçš„ç·¨ç¢¼æ–¹å¼è§£ç¢¼
            encodings_to_try = [encoding, 'utf-8', 'big5', 'gbk', 'cp1252', 'latin1']
            decoded_data = None
            
            for enc in encodings_to_try:
                if enc is None:
                    continue
                try:
                    decoded_data = stdout_data.decode(enc)
                    logger.info(f"æˆåŠŸä½¿ç”¨ç·¨ç¢¼ {enc} è§£ç¢¼")
                    break
                except (UnicodeDecodeError, LookupError):
                    continue
            
            if decoded_data is None:
                # æœ€å¾Œæ‰‹æ®µï¼šä½¿ç”¨ errors='replace' å¼·åˆ¶è§£ç¢¼
                decoded_data = stdout_data.decode('utf-8', errors='replace')
                logger.warning(f"ä½¿ç”¨ UTF-8 å¼·åˆ¶è§£ç¢¼ï¼Œå¯èƒ½æœ‰è³‡æ–™éºå¤±")
            
            # è¨­å®šç•¶å‰è¡¨æ ¼åç¨±ï¼Œä¾› _simple_process_csv ä½¿ç”¨
            self._current_table_name = table_name
            
            # ç°¡åŒ–è™•ç†ï¼šç›´æ¥ç”¨ UTF-8 é‡æ–°è§£ç¢¼ä¸¦éæ¿¾æ¬„ä½
            processed_data = self._simple_process_csv(decoded_data)
            
            # å¯«å…¥è™•ç†å¾Œçš„ CSV æª”æ¡ˆ
            with open(output_file, 'w', newline='', encoding='utf-8') as f:
                f.write(processed_data)
            
            # è¨ˆç®—è¡Œæ•¸
            line_count = processed_data.count('\n')
            
            logger.info(f"âœ… æˆåŠŸåŒ¯å‡º {table_name} -> {output_file.name} ({line_count} è¡Œ)")
            return True
                    
        except Exception as e:
            logger.error(f"âŒ åŒ¯å‡º {table_name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False
    
    def _process_large_csv(self, csv_file):
        """è™•ç†å¤§å‹ CSV æª”æ¡ˆï¼Œé€²è¡Œå„ªåŒ–"""
        try:
            file_size = os.path.getsize(csv_file)
            
            # å¦‚æœæª”æ¡ˆå°æ–¼ 100MBï¼Œç›´æ¥è¿”å›
            if file_size < 100 * 1024 * 1024:
                return
            
            logger.info(f"è™•ç†å¤§å‹æª”æ¡ˆ {csv_file.name} ({file_size / 1024 / 1024:.1f} MB)")
            
            # æª¢æ¸¬ç·¨ç¢¼
            with open(csv_file, 'rb') as f:
                raw_data = f.read(10000)
                encoding = chardet.detect(raw_data)['encoding']
            
            # åˆ†å¡Šè™•ç†å¤§æª”æ¡ˆ
            temp_file = csv_file.with_suffix('.tmp')
            
            try:
                chunk_iter = pd.read_csv(csv_file, encoding=encoding, chunksize=self.chunk_size)
                
                first_chunk = True
                for chunk in chunk_iter:
                    # æ¸…ç†è³‡æ–™
                    chunk = chunk.dropna(how='all')  # ç§»é™¤å®Œå…¨ç©ºç™½çš„è¡Œ
                    chunk = chunk.fillna('')  # å¡«å…… NaN å€¼
                    
                    # å¯«å…¥è‡¨æ™‚æª”æ¡ˆ
                    chunk.to_csv(
                        temp_file, 
                        mode='a' if not first_chunk else 'w',
                        header=first_chunk,
                        index=False,
                        encoding='utf-8'
                    )
                    first_chunk = False
                
                # æ›¿æ›åŸæª”æ¡ˆ
                temp_file.replace(csv_file)
                logger.info(f"âœ… å¤§å‹æª”æ¡ˆè™•ç†å®Œæˆ: {csv_file.name}")
                
            except Exception as chunk_error:
                logger.warning(f"âš ï¸ ç„¡æ³•åˆ†å¡Šè™•ç†æª”æ¡ˆ {csv_file.name}: {chunk_error}")
                # å¦‚æœåˆ†å¡Šè™•ç†å¤±æ•—ï¼Œä¿ç•™åŸæª”æ¡ˆ
                if temp_file.exists():
                    temp_file.unlink()
            
        except Exception as e:
            logger.error(f"âŒ è™•ç†å¤§å‹æª”æ¡ˆ {csv_file.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def _save_compressed_file(self, compressed_data, record_id, table_name):
        """ä¿å­˜å£“ç¸®æª”æ¡ˆåˆ°ç£ç¢Ÿ"""
        try:
            if not compressed_data:
                return None
            
            # ä½¿ç”¨ magic åˆ¤æ–·æª”æ¡ˆé¡å‹
            mime_type = magic.from_buffer(compressed_data, mime=True)
            logger.debug(f"åµæ¸¬åˆ° MIME type: {mime_type}")
            
            # æ ¹æ“š MIME type å’Œæª”æ¡ˆå…§å®¹ç¢ºå®šå‰¯æª”å
            extension_map = {
                'application/zip': '.zip',
                'application/x-zip-compressed': '.zip',
                'application/octet-stream': '.bin',
                'text/plain': '.txt',
                'application/pdf': '.pdf',
                'application/msword': '.doc',
                'application/vnd.openxmlformats-officedocument.wordprocessingml.document': '.docx',
                'image/jpeg': '.jpg',
                'image/png': '.png',
            }
            
            # æª¢æŸ¥æª”æ¡ˆæ¨™é ­ä¾†è¦†è“‹MIMEæª¢æ¸¬çµæœ
            if compressed_data[:2] == b'PK':
                extension = '.zip'
            elif (compressed_data[:8] == b'\x89PNG\r\n\x1a\n' or 
                  b'PNG' in compressed_data[:20] or 
                  b'IHDR' in compressed_data[:20]):
                extension = '.png'
            elif compressed_data[:2] == b'\xff\xd8':
                extension = '.jpg'
            else:
                extension = extension_map.get(mime_type, '.bin')
            
            # å»ºç«‹æª”æ¡ˆè·¯å¾‘
            safe_table_name = table_name.replace('/', '_').replace('\\', '_')
            filename = f"{safe_table_name}_{record_id}{extension}"
            file_path = self.extracted_files_dir / filename
            
            # ä¿å­˜æª”æ¡ˆ
            with open(file_path, 'wb') as f:
                f.write(compressed_data)
            
            logger.info(f"å·²ä¿å­˜å£“ç¸®æª”æ¡ˆ: {filename} (MIME: {mime_type}, å¤§å°: {len(compressed_data)} bytes)")
            return file_path
            
        except Exception as e:
            logger.error(f"ä¿å­˜å£“ç¸®æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None
    
    def _extract_compressed_text(self, compressed_data, record_id=None, table_name=None):
        """è§£å£“ç¸® Word æ¬„ä½ä¸­çš„æ–‡å­—å…§å®¹ä¸¦ä¿å­˜åŸå§‹æª”æ¡ˆ"""
        try:
            if not compressed_data or len(compressed_data) < 2:
                return ""
            
            # å°‡å­—ä¸²è½‰æ›ç‚º bytes (å¦‚æœéœ€è¦)
            if isinstance(compressed_data, str):
                # å˜—è©¦ä¸åŒæ–¹æ³•å°‡å­—ä¸²è½‰ç‚ºåŸå§‹äºŒé€²ä½è³‡æ–™
                binary_data = None
                
                # æ–¹æ³• 1: å˜—è©¦ latin1 ç·¨ç¢¼ (ä¿ç•™åŸå§‹ä½å…ƒçµ„)
                try:
                    binary_data = compressed_data.encode('latin1')
                    logger.debug(f"ä½¿ç”¨ latin1 ç·¨ç¢¼è½‰æ›ï¼Œè³‡æ–™å¤§å°: {len(binary_data)} bytes")
                except UnicodeEncodeError:
                    pass
                
                # æ–¹æ³• 2: å¦‚æœåŒ…å«ç„¡æ³•ç”¨ latin1 ç·¨ç¢¼çš„å­—ç¬¦ï¼Œå˜—è©¦ base64 è§£ç¢¼
                if binary_data is None:
                    try:
                        # ç§»é™¤å¯èƒ½çš„ç©ºç™½å­—ç¬¦
                        clean_data = ''.join(compressed_data.split())
                        binary_data = base64.b64decode(clean_data)
                        logger.debug(f"ä½¿ç”¨ base64 è§£ç¢¼ï¼Œè³‡æ–™å¤§å°: {len(binary_data)} bytes")
                    except:
                        pass
                
                # æ–¹æ³• 3: å˜—è©¦ hex è§£ç¢¼
                if binary_data is None:
                    try:
                        # å‡è¨­è³‡æ–™æ˜¯åå…­é€²ä½å­—ä¸²
                        clean_hex = ''.join(c for c in compressed_data if c in '0123456789abcdefABCDEF')
                        if len(clean_hex) % 2 == 0 and len(clean_hex) > 0:
                            binary_data = bytes.fromhex(clean_hex)
                            logger.debug(f"ä½¿ç”¨ hex è§£ç¢¼ï¼Œè³‡æ–™å¤§å°: {len(binary_data)} bytes")
                    except:
                        pass
                
                # æ–¹æ³• 4: æœ€å¾Œæ‰‹æ®µï¼Œä½¿ç”¨ UTF-8 ä¸¦å¿½ç•¥éŒ¯èª¤
                if binary_data is None:
                    binary_data = compressed_data.encode('utf-8', errors='ignore')
                    logger.debug(f"ä½¿ç”¨ UTF-8 å¼·åˆ¶ç·¨ç¢¼ï¼Œè³‡æ–™å¤§å°: {len(binary_data)} bytes")
                    
            else:
                binary_data = compressed_data
                logger.debug(f"è¼¸å…¥å·²ç¶“æ˜¯äºŒé€²ä½è³‡æ–™ï¼Œå¤§å°: {len(binary_data)} bytes")
            
            # ä½¿ç”¨ magic åˆ¤æ–·æª”æ¡ˆé¡å‹
            mime_type = magic.from_buffer(binary_data, mime=True)
            logger.debug(f"åµæ¸¬åˆ° MIME type: {mime_type}")
            
            # ä¿å­˜åŸå§‹æª”æ¡ˆ
            if record_id and table_name:
                saved_path = self._save_compressed_file(binary_data, record_id, table_name)
                if saved_path:
                    logger.debug(f"åŸå§‹æª”æ¡ˆå·²ä¿å­˜è‡³: {saved_path}")
            
            extracted_text = ""
            
            # æ ¹æ“š MIME type å’Œæª”æ¡ˆæ¨™é ­è™•ç†ä¸åŒé¡å‹çš„æª”æ¡ˆ
            
            # æª¢æŸ¥æ˜¯å¦ç‚º ZIP æª”æ¡ˆ
            is_zip = (mime_type in ['application/zip', 'application/x-zip-compressed'] or 
                     binary_data[:2] == b'PK' or  # ZIP é­”è¡“æ•¸å­—
                     binary_data[:4] == b'PK\x03\x04')  # ZIP æœ¬åœ°æª”æ¡ˆæ¨™é ­
            
            # æª¢æŸ¥æ˜¯å¦ç‚º PNG æª”æ¡ˆï¼ˆå®Œæ•´æˆ–ç‰‡æ®µï¼‰
            is_png = (mime_type == 'image/png' or 
                     binary_data[:8] == b'\x89PNG\r\n\x1a\n' or  # å®Œæ•´PNGæ¨™é ­
                     b'PNG' in binary_data[:20] or  # PNGæ¨™è­˜
                     b'IHDR' in binary_data[:20])  # PNGåœ–ç‰‡æ¨™é ­å¡Š
            
            # æª¢æŸ¥æ˜¯å¦ç‚º JPEG æª”æ¡ˆ
            is_jpeg = (mime_type in ['image/jpeg', 'image/jpg'] or
                      binary_data[:2] == b'\xff\xd8')  # JPEGæ¨™é ­
            
            if is_zip:
                try:
                    with zipfile.ZipFile(io.BytesIO(binary_data)) as zip_file:
                        logger.info(f"ZIP æª”æ¡ˆåŒ…å«ä»¥ä¸‹æª”æ¡ˆ: {zip_file.namelist()}")
                        
                        # å˜—è©¦è§£å£“ç¸®æ‰€æœ‰æ–‡å­—æª”æ¡ˆ
                        text_parts = []
                        
                        for file_name in zip_file.namelist():
                            try:
                                file_content = zip_file.read(file_name)
                                file_mime = magic.from_buffer(file_content, mime=True)
                                
                                logger.debug(f"ZIP å…§æª”æ¡ˆ {file_name} çš„ MIME type: {file_mime}")
                                
                                # å¦‚æœæ˜¯æ–‡å­—æª”æ¡ˆï¼Œå˜—è©¦è§£ç¢¼
                                if file_mime.startswith('text/'):
                                    for encoding in ['utf-8', 'big5', 'gbk', 'cp1252', 'latin1']:
                                        try:
                                            text = file_content.decode(encoding).strip()
                                            if text:
                                                text_parts.append(f"[{file_name}]: {text}")
                                                break
                                        except UnicodeDecodeError:
                                            continue
                                else:
                                    # éæ–‡å­—æª”æ¡ˆï¼Œè¨˜éŒ„æª”æ¡ˆè³‡è¨Š
                                    text_parts.append(f"[{file_name}]: éæ–‡å­—æª”æ¡ˆ ({file_mime}, {len(file_content)} bytes)")
                                    
                            except Exception as e:
                                logger.warning(f"ç„¡æ³•è®€å– ZIP å…§æª”æ¡ˆ {file_name}: {e}")
                                text_parts.append(f"[{file_name}]: è®€å–éŒ¯èª¤")
                        
                        extracted_text = "\n".join(text_parts)
                        
                except zipfile.BadZipFile as e:
                    logger.warning(f"ZIP æª”æ¡ˆæ ¼å¼éŒ¯èª¤: {e}")
                    extracted_text = f"[ZIP æ ¼å¼éŒ¯èª¤: {e}]"
                except Exception as e:
                    logger.warning(f"è™•ç† ZIP æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    extracted_text = f"[ZIP è™•ç†éŒ¯èª¤: {e}]"
            
            elif is_png:
                try:
                    # PNG åœ–ç‰‡æª”æ¡ˆ
                    extracted_text = f"[PNGåœ–ç‰‡: {len(binary_data)} bytes"
                    
                    # å˜—è©¦å¾PNGä¸­æå–åŸºæœ¬è³‡è¨Š
                    if b'IHDR' in binary_data:
                        ihdr_pos = binary_data.find(b'IHDR')
                        if ihdr_pos >= 0 and len(binary_data) > ihdr_pos + 12:
                                                         # PNG IHDR çµæ§‹ï¼šé•·åº¦(4) + "IHDR"(4) + å¯¬åº¦(4) + é«˜åº¦(4)
                             width_bytes = binary_data[ihdr_pos+4:ihdr_pos+8]
                             height_bytes = binary_data[ihdr_pos+8:ihdr_pos+12]
                             if len(width_bytes) == 4 and len(height_bytes) == 4:
                                 try:
                                     import struct
                                     width = struct.unpack('>I', width_bytes)[0]
                                     height = struct.unpack('>I', height_bytes)[0]
                                     # é©—è­‰åƒç´ å€¼æ˜¯å¦åˆç†ï¼ˆé¿å…éŒ¯èª¤è§£æï¼‰
                                     if width > 0 and height > 0 and width < 100000 and height < 100000:
                                         extracted_text += f", {width}x{height}åƒç´ "
                                     else:
                                         extracted_text += ", åƒç´ è³‡è¨Šä¸å®Œæ•´"
                                 except:
                                     extracted_text += ", åƒç´ è§£æå¤±æ•—"
                    
                    extracted_text += "]"
                    
                except Exception as e:
                    logger.warning(f"è™•ç†PNGæª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    extracted_text = f"[PNGæª”æ¡ˆè™•ç†éŒ¯èª¤: {len(binary_data)} bytes]"
            
            elif is_jpeg:
                # JPEG åœ–ç‰‡æª”æ¡ˆ
                extracted_text = f"[JPEGåœ–ç‰‡: {len(binary_data)} bytes]"
            
            elif mime_type.startswith('text/'):
                # ç´”æ–‡å­—æª”æ¡ˆ
                for encoding in ['utf-8', 'big5', 'gbk', 'cp1252', 'latin1']:
                    try:
                        extracted_text = binary_data.decode(encoding).strip()
                        if extracted_text:
                            break
                    except UnicodeDecodeError:
                        continue
            
            else:
                # å…¶ä»–é¡å‹çš„æª”æ¡ˆ
                extracted_text = f"[äºŒé€²ä½æª”æ¡ˆ: {mime_type}, {len(binary_data)} bytes]"
            
            # å¦‚æœæ²’æœ‰æå–åˆ°æ–‡å­—ï¼Œå˜—è©¦å¼·åˆ¶è§£ç¢¼
            if not extracted_text:
                extracted_text = binary_data.decode('utf-8', errors='replace').strip()
                if not extracted_text or len(extracted_text.strip()) < 3:
                    extracted_text = f"[ç„¡æ³•è§£ç¢¼: {mime_type}, {len(binary_data)} bytes]"
            
            return extracted_text
            
        except Exception as e:
            logger.error(f"è§£å£“ç¸®æ–‡å­—å…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return f"[è§£æéŒ¯èª¤: {e}]"
    
    def _simple_process_csv(self, raw_data):
        """ç°¡åŒ–çš„ CSV è™•ç†ï¼šåªä¿ç•™æ–‡å­—æ¬„ä½ï¼Œå¼·åˆ¶ UTF-8 è§£ç¢¼"""
        try:
            lines = raw_data.split('\n')
            if not lines:
                return ""
            
            # è™•ç†æ¨™é ­
            header_line = lines[0]
            headers = [h.strip().strip('"') for h in header_line.split(',')]
            logger.info(f"åŸå§‹æ¬„ä½: {headers}")
            
            # æ‰¾åˆ°è¦ä¿ç•™çš„æ¬„ä½ï¼ˆè·³éåœ–ç‰‡æ¬„ä½ï¼‰
            keep_headers = []
            keep_indices = []
            
            for i, header in enumerate(headers):
                if header.lower() not in ['qpic', 'apic']:
                    keep_headers.append(header)
                    keep_indices.append(i)
            
            logger.info(f"ä¿ç•™æ¬„ä½: {keep_headers}")
            
            # å»ºç«‹è¼¸å‡º
            output_lines = [','.join(f'"{h}"' for h in keep_headers)]
            
            # è™•ç†è³‡æ–™è¡Œ - ä½¿ç”¨ç°¡å–®çš„æ–¹æ³•
            processed_count = 0
            for line_num, line in enumerate(lines[1:], 1):
                if not line.strip():
                    continue
                
                try:
                    # ç°¡å–®åˆ†å‰²ï¼ˆå¯èƒ½ä¸å®Œç¾ï¼Œä½†é¿å…äºŒé€²ä½å•é¡Œï¼‰
                    parts = line.split('","')
                    
                    # æ¸…ç†å¼•è™Ÿ
                    if parts:
                        parts[0] = parts[0].lstrip('"')
                        parts[-1] = parts[-1].rstrip('"')
                    
                    # åªä¿ç•™éœ€è¦çš„æ¬„ä½
                    row_data = []
                    for idx in keep_indices:
                        if idx < len(parts):
                            value = parts[idx]
                            
                            # å° Word æ¬„ä½é€²è¡Œç‰¹æ®Šè™•ç†
                            if idx < len(headers) and headers[idx].lower() == 'word':
                                # ä½¿ç”¨æ–°çš„è§£å£“ç¸®æ–¹æ³•è™•ç† Word æ¬„ä½
                                try:
                                    # ç‚ºæ¯å€‹è¨˜éŒ„ç”Ÿæˆå”¯ä¸€ ID (ä½¿ç”¨è¡Œè™Ÿ)
                                    record_id = f"row_{line_num}"
                                    # ä½¿ç”¨ç•¶å‰æª”æ¡ˆåä½œç‚º table_name
                                    table_name = getattr(self, '_current_table_name', 'unknown')
                                    
                                    # æª¢æŸ¥æ˜¯å¦æœ‰å¯¦éš›å…§å®¹ï¼ˆæ”¹ç‚ºæ›´å¯¬é¬†çš„æ¢ä»¶ï¼‰
                                    if value and len(value.strip()) > 5:
                                        # å˜—è©¦è§£å£“ç¸®å…§å®¹
                                        extracted_text = self._extract_compressed_text(value, record_id, table_name)
                                        value = extracted_text if extracted_text else "[ç„¡å…§å®¹]"
                                        logger.debug(f"è™•ç†äº† Word æ¬„ä½ (è¡Œ {line_num}): {len(value)} å­—å…ƒ")
                                    else:
                                        value = ""
                                        logger.debug(f"è·³é Word æ¬„ä½ (è¡Œ {line_num}): å…§å®¹å¤ªçŸ­ ({len(value) if value else 0} å­—å…ƒ)")
                                except Exception as e:
                                    logger.warning(f"è™•ç† Word æ¬„ä½æ™‚ç™¼ç”ŸéŒ¯èª¤ (è¡Œ {line_num}): {e}")
                                    value = f"[è™•ç†éŒ¯èª¤: {e}]"
                            else:
                                # ä¸€èˆ¬æ¬„ä½ä¹Ÿåšæ¸…ç†
                                try:
                                    value = value.encode('utf-8', errors='ignore').decode('utf-8', errors='ignore')
                                    value = ''.join(char for char in value if char.isprintable() or char.isspace())
                                except:
                                    value = ""
                            
                            # è½‰ç¾©å¼•è™Ÿä¸¦åŠ å…¥
                            value = str(value).replace('"', '""')
                            row_data.append(f'"{value}"')
                        else:
                            row_data.append('""')
                    
                    if row_data:
                        output_lines.append(','.join(row_data))
                        processed_count += 1
                    
                    # é¡¯ç¤ºé€²åº¦
                    if line_num % 50000 == 0:
                        logger.info(f"å·²è™•ç† {line_num} è¡Œï¼Œæœ‰æ•ˆè³‡æ–™ {processed_count} è¡Œ")
                
                except Exception as e:
                    logger.debug(f"è·³éç¬¬ {line_num} è¡Œ: {e}")
                    continue
            
            result = '\n'.join(output_lines)
            logger.info(f"ç°¡åŒ–è™•ç†å®Œæˆï¼Œè¼¸å‡º {len(output_lines)} è¡Œ (æœ‰æ•ˆè³‡æ–™ {processed_count} è¡Œ)")
            return result
            
        except Exception as e:
            logger.error(f"ç°¡åŒ–è™•ç†å¤±æ•—: {e}")
            # æœ€å¾Œæ‰‹æ®µï¼šåªè¿”å›æ¨™é ­
            try:
                headers = [h.strip().strip('"') for h in raw_data.split('\n')[0].split(',')]
                clean_headers = [h for h in headers if h.lower() not in ['qpic', 'apic']]
                return ','.join(f'"{h}"' for h in clean_headers)
            except:
                return "Code,Word,LessonID"
    
    def _process_csv_data(self, raw_data):
        """è™•ç† CSV è³‡æ–™ï¼Œè·³éåœ–ç‰‡æ¬„ä½ä¸¦è§£å£“ç¸®æ–‡å­—"""
        try:
            # æ‰¾åˆ°æ¨™é ­è¡Œ
            lines = raw_data.split('\n', 1)  # åªåˆ†å‰²ç¬¬ä¸€è¡Œ
            if len(lines) < 2:
                return ""
            
            header_line = lines[0]
            data_content = lines[1]
            
            # è§£ææ¨™é ­ - æ¨™é ­æ‡‰è©²æ˜¯ä¹¾æ·¨çš„
            headers = [h.strip('"') for h in header_line.split(',')]
            logger.info(f"æ‰¾åˆ°æ¬„ä½: {headers}")
            
            # æ‰¾åˆ°éœ€è¦ä¿ç•™çš„æ¬„ä½ç´¢å¼•
            keep_indices = []
            keep_headers = []
            
            for i, header in enumerate(headers):
                header_lower = header.lower().strip()
                if header_lower not in ['qpic', 'apic']:  # è·³éåœ–ç‰‡æ¬„ä½
                    keep_indices.append(i)
                    keep_headers.append(header)
            
            logger.info(f"ä¿ç•™æ¬„ä½: {keep_headers}")
            
            # å»ºç«‹è¼¸å‡ºæ¨™é ­
            output_lines = [','.join(f'"{h}"' for h in keep_headers)]
            
            # ä½¿ç”¨æ›´ç²¾ç¢ºçš„æ–¹æ³•è§£ææ¯ä¸€è¡Œ
            # æ‰‹å‹•è§£æ CSVï¼Œå› ç‚ºåŒ…å«äºŒé€²ä½è³‡æ–™
            line_num = 0
            pos = 0
            
            while pos < len(data_content):
                line_num += 1
                
                # å°‹æ‰¾ä¸‹ä¸€è¡Œçš„é–‹å§‹
                if data_content[pos] != '"':
                    # å°‹æ‰¾ä¸‹ä¸€å€‹å¼•è™Ÿé–‹å§‹
                    next_quote = data_content.find('"', pos)
                    if next_quote == -1:
                        break
                    pos = next_quote
                
                # è§£æç•¶å‰è¡Œçš„æ‰€æœ‰æ¬„ä½
                row_values = []
                field_index = 0
                
                while field_index < len(headers) and pos < len(data_content):
                    if data_content[pos] == '"':
                        # æ‰¾åˆ°æ¬„ä½å…§å®¹çš„çµæŸ
                        pos += 1  # è·³éé–‹å§‹çš„å¼•è™Ÿ
                        field_start = pos
                        
                        # å°‹æ‰¾æ¬„ä½çµæŸçš„å¼•è™Ÿ
                        quote_count = 0
                        while pos < len(data_content):
                            if data_content[pos] == '"':
                                # æª¢æŸ¥æ˜¯å¦ç‚ºè½‰ç¾©çš„å¼•è™Ÿ
                                if pos + 1 < len(data_content) and data_content[pos + 1] == '"':
                                    pos += 2  # è·³éè½‰ç¾©çš„å¼•è™Ÿ
                                    continue
                                else:
                                    # æ‰¾åˆ°æ¬„ä½çµæŸ
                                    field_value = data_content[field_start:pos]
                                    row_values.append(field_value)
                                    pos += 1  # è·³éçµæŸå¼•è™Ÿ
                                    break
                            else:
                                pos += 1
                        
                        # è·³éé€—è™Ÿæˆ–æ›è¡Œ
                        if pos < len(data_content) and data_content[pos] == ',':
                            pos += 1
                        elif pos < len(data_content) and data_content[pos] in '\r\n':
                            # è·³éæ›è¡Œç¬¦è™Ÿ
                            if data_content[pos] == '\r' and pos + 1 < len(data_content) and data_content[pos + 1] == '\n':
                                pos += 2
                            else:
                                pos += 1
                            break
                    else:
                        # è™•ç†æ²’æœ‰å¼•è™Ÿçš„æ¬„ä½ï¼ˆä¸å¤ªå¯èƒ½åœ¨é€™å€‹è³‡æ–™é›†ä¸­ï¼‰
                        field_start = pos
                        while pos < len(data_content) and data_content[pos] not in ',\r\n':
                            pos += 1
                        field_value = data_content[field_start:pos]
                        row_values.append(field_value)
                        
                        if pos < len(data_content) and data_content[pos] == ',':
                            pos += 1
                        elif pos < len(data_content) and data_content[pos] in '\r\n':
                            if data_content[pos] == '\r' and pos + 1 < len(data_content) and data_content[pos + 1] == '\n':
                                pos += 2
                            else:
                                pos += 1
                            break
                    
                    field_index += 1
                
                # è™•ç†é€™ä¸€è¡Œçš„è³‡æ–™
                if row_values:
                    processed_row = []
                    for i in keep_indices:
                        if i < len(row_values):
                            value = row_values[i]
                            
                            # å¦‚æœæ˜¯ Word æ¬„ä½ï¼Œå˜—è©¦è§£å£“ç¸®
                            if headers[i].lower().strip() == 'word' and value:
                                try:
                                    # æª¢æŸ¥æ˜¯å¦ç‚º ZIP æ ¼å¼
                                    if len(value) > 2:
                                        binary_data = value.encode('latin1', errors='ignore')
                                        if binary_data[:2] == b'PK':
                                            extracted_text = self._extract_compressed_text(binary_data)
                                            if extracted_text:
                                                value = extracted_text
                                            else:
                                                value = f"[å£“ç¸®æ–‡å­—_{line_num}]"
                                except Exception as e:
                                    logger.warning(f"è§£å£“ç¸®ç¬¬ {line_num} è¡Œæ–‡å­—æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                                    value = f"[è§£æéŒ¯èª¤_{line_num}]"
                            
                            # æ¸…ç†å€¼ä¸¦è½‰ç¾©
                            value = str(value).replace('"', '""')
                            processed_row.append(f'"{value}"')
                        else:
                            processed_row.append('""')
                    
                    if processed_row:
                        output_lines.append(','.join(processed_row))
                
                # é¡¯ç¤ºé€²åº¦
                if line_num % 10000 == 0:
                    logger.info(f"å·²è™•ç† {line_num} è¡Œ...")
            
            result = '\n'.join(output_lines)
            logger.info(f"è™•ç†å®Œæˆï¼Œè¼¸å‡º {len(output_lines)} è¡Œ")
            return result
            
        except Exception as e:
            logger.error(f"è™•ç† CSV è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            # è¿”å›åƒ…åŒ…å«æ¨™é ­çš„ç°¡åŒ–ç‰ˆæœ¬
            try:
                lines = raw_data.split('\n')
                if lines:
                    headers = [h.strip('"') for h in lines[0].split(',')]
                    keep_headers = [h for h in headers if h.lower() not in ['qpic', 'apic']]
                    return ','.join(f'"{h}"' for h in keep_headers)
            except:
                pass
            return raw_data
    
    def convert_single_mdb(self, mdb_file):
        """è½‰æ›å–®å€‹ MDB æª”æ¡ˆ"""
        logger.info(f"é–‹å§‹è™•ç† MDB æª”æ¡ˆ: {mdb_file.name}")
        
        # å–å¾—è¡¨æ ¼åˆ—è¡¨
        tables = self._get_table_list(mdb_file)
        if not tables:
            logger.warning(f"åœ¨ {mdb_file.name} ä¸­æ‰¾ä¸åˆ°è¡¨æ ¼")
            return False
        
        logger.info(f"æ‰¾åˆ° {len(tables)} å€‹è¡¨æ ¼: {tables}")
        
        # ç‚ºæ¯å€‹ MDB æª”æ¡ˆå»ºç«‹å°æ‡‰çš„è¼¸å‡ºç›®éŒ„
        mdb_output_dir = self.output_dir / mdb_file.stem
        mdb_output_dir.mkdir(exist_ok=True)
        
        success_count = 0
        
        # è™•ç†æ¯å€‹è¡¨æ ¼
        for table in tqdm(tables, desc=f"è™•ç† {mdb_file.name}"):
            output_file = mdb_output_dir / f"{table}.csv"
            
            if self._export_table_to_csv(mdb_file, table, output_file):
                # è™•ç†å¤§å‹ CSV æª”æ¡ˆ
                self._process_large_csv(output_file)
                success_count += 1
        
        logger.info(f"å®Œæˆè™•ç† {mdb_file.name}: {success_count}/{len(tables)} å€‹è¡¨æ ¼æˆåŠŸè½‰æ›")
        return success_count == len(tables)
    
    def convert_all_mdb_files(self):
        """æ‰¹æ¬¡è½‰æ›æ‰€æœ‰ MDB æª”æ¡ˆ"""
        mdb_files = self._get_mdb_files()
        
        if not mdb_files:
            logger.error("æ²’æœ‰æ‰¾åˆ°è¦è™•ç†çš„ MDB æª”æ¡ˆ")
            return
        
        logger.info(f"æ‰¾åˆ° {len(mdb_files)} å€‹ MDB æª”æ¡ˆ")
        
        # ä½¿ç”¨åŸ·è¡Œç·’æ± é€²è¡Œä½µç™¼è™•ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_file = {
                executor.submit(self.convert_single_mdb, mdb_file): mdb_file 
                for mdb_file in mdb_files
            }
            
            successful_conversions = 0
            
            for future in as_completed(future_to_file):
                mdb_file = future_to_file[future]
                try:
                    success = future.result()
                    if success:
                        successful_conversions += 1
                except Exception as e:
                    logger.error(f"è™•ç† {mdb_file.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        logger.info(f"è½‰æ›å®Œæˆï¼æˆåŠŸè™•ç† {successful_conversions}/{len(mdb_files)} å€‹æª”æ¡ˆ")
    
    def get_conversion_summary(self):
        """å–å¾—è½‰æ›æ‘˜è¦è³‡è¨Š"""
        summary = {
            'total_csv_files': 0,
            'total_size_mb': 0,
            'directories': [],
            'extracted_files': {
                'count': 0,
                'size_mb': 0,
                'types': {}
            }
        }
        
        for item in self.output_dir.iterdir():
            if item.is_dir() and item.name != 'extracted_files':
                csv_files = list(item.glob("*.csv"))
                dir_size = sum(f.stat().st_size for f in csv_files)
                
                summary['directories'].append({
                    'name': item.name,
                    'csv_count': len(csv_files),
                    'size_mb': dir_size / 1024 / 1024
                })
                
                summary['total_csv_files'] += len(csv_files)
                summary['total_size_mb'] += dir_size / 1024 / 1024
        
        # çµ±è¨ˆè§£å£“ç¸®æª”æ¡ˆ
        if self.extracted_files_dir.exists():
            extracted_files = list(self.extracted_files_dir.glob("*"))
            summary['extracted_files']['count'] = len(extracted_files)
            
            total_extracted_size = 0
            for file_path in extracted_files:
                if file_path.is_file():
                    file_size = file_path.stat().st_size
                    total_extracted_size += file_size
                    
                    # çµ±è¨ˆæª”æ¡ˆé¡å‹
                    extension = file_path.suffix.lower()
                    if extension in summary['extracted_files']['types']:
                        summary['extracted_files']['types'][extension] += 1
                    else:
                        summary['extracted_files']['types'][extension] = 1
            
            summary['extracted_files']['size_mb'] = total_extracted_size / 1024 / 1024
        
        return summary

def main():
    parser = argparse.ArgumentParser(description='MDB to CSV æ‰¹æ¬¡è½‰æ›å™¨')
    parser.add_argument('--input_dir', '-i', required=True, help='MDB æª”æ¡ˆè¼¸å…¥ç›®éŒ„')
    parser.add_argument('--output_dir', '-o', required=True, help='CSV æª”æ¡ˆè¼¸å‡ºç›®éŒ„')
    parser.add_argument('--max_workers', '-w', type=int, default=4, help='æœ€å¤§ä½µç™¼æ•¸ (é è¨­: 4)')
    parser.add_argument('--chunk_size', '-c', type=int, default=10000, help='å¤§æª”æ¡ˆè™•ç†åˆ†å¡Šå¤§å° (é è¨­: 10000)')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.input_dir):
        logger.error(f"è¼¸å…¥ç›®éŒ„ä¸å­˜åœ¨: {args.input_dir}")
        sys.exit(1)
    
    # å»ºç«‹è½‰æ›å™¨ä¸¦åŸ·è¡Œè½‰æ›
    converter = MDBToCSVConverter(
        args.input_dir, 
        args.output_dir, 
        args.max_workers, 
        args.chunk_size
    )
    
    # åŸ·è¡Œè½‰æ›
    converter.convert_all_mdb_files()
    
    # é¡¯ç¤ºæ‘˜è¦
    summary = converter.get_conversion_summary()
    logger.info("\n=== è½‰æ›æ‘˜è¦ ===")
    logger.info(f"ç¸½å…±ç”¢ç”Ÿ {summary['total_csv_files']} å€‹ CSV æª”æ¡ˆ")
    logger.info(f"ç¸½å¤§å°: {summary['total_size_mb']:.1f} MB")
    
    for dir_info in summary['directories']:
        logger.info(f"  ğŸ“ {dir_info['name']}: {dir_info['csv_count']} å€‹æª”æ¡ˆ, {dir_info['size_mb']:.1f} MB")
    
    # é¡¯ç¤ºè§£å£“ç¸®æª”æ¡ˆæ‘˜è¦
    extracted = summary['extracted_files']
    if extracted['count'] > 0:
        logger.info(f"\n=== è§£å£“ç¸®æª”æ¡ˆæ‘˜è¦ ===")
        logger.info(f"ç¸½å…±è§£å£“ç¸® {extracted['count']} å€‹æª”æ¡ˆ")
        logger.info(f"è§£å£“ç¸®æª”æ¡ˆç¸½å¤§å°: {extracted['size_mb']:.1f} MB")
        logger.info("æª”æ¡ˆé¡å‹åˆ†ä½ˆ:")
        for ext, count in extracted['types'].items():
            logger.info(f"  {ext if ext else 'ç„¡å‰¯æª”å'}: {count} å€‹æª”æ¡ˆ")

if __name__ == "__main__":
    main() 